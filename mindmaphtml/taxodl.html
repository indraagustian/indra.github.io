<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.10/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.10/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.10/dist/index.js"></script><script>(r => {
              setTimeout(r);
            })(function renderToolbar() {
  const {
    markmap,
    mm
  } = window;
  const {
    el
  } = markmap.Toolbar.create(mm);
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root2, jsonOptions) => {
              const markmap = getMarkmap();
              window.mm = markmap.Markmap.create(
                "svg#mindmap",
                (getOptions || markmap.deriveOptions)(jsonOptions),
                root2
              );
            })(() => window.markmap,null,{"content":"Taxonomy: Optimization in Deep Learning","children":[{"content":"1. Optimization Targets","children":[{"content":"Loss Function","children":[{"content":"Classification Loss","children":[{"content":"Cross-Entropy","children":[],"payload":{"tag":"li","lines":"14,15"}},{"content":"Hinge Loss","children":[],"payload":{"tag":"li","lines":"15,16"}}],"payload":{"tag":"li","lines":"13,16"}},{"content":"Regression Loss","children":[{"content":"Mean Squared Error (MSE)","children":[],"payload":{"tag":"li","lines":"17,18"}},{"content":"Mean Absolute Error (MAE)","children":[],"payload":{"tag":"li","lines":"18,19"}}],"payload":{"tag":"li","lines":"16,19"}},{"content":"Embedding Loss","children":[{"content":"Triplet Loss","children":[],"payload":{"tag":"li","lines":"20,21"}},{"content":"Contrastive Loss","children":[],"payload":{"tag":"li","lines":"21,22"}}],"payload":{"tag":"li","lines":"19,22"}}],"payload":{"tag":"li","lines":"12,22"}},{"content":"Accuracy/Performance Metrics","children":[{"content":"Precision, Recall, F1","children":[],"payload":{"tag":"li","lines":"23,24"}},{"content":"Top-k Accuracy","children":[],"payload":{"tag":"li","lines":"24,26"}}],"payload":{"tag":"li","lines":"22,26"}}],"payload":{"tag":"h2","lines":"11,12"}},{"content":"2. Optimizer Algorithms","children":[{"content":"First-Order Methods","children":[{"content":"Gradient Descent (GD)","children":[],"payload":{"tag":"li","lines":"28,29"}},{"content":"Stochastic Gradient Descent (SGD)","children":[],"payload":{"tag":"li","lines":"29,30"}},{"content":"Mini-Batch SGD","children":[],"payload":{"tag":"li","lines":"30,31"}}],"payload":{"tag":"li","lines":"27,31"}},{"content":"Adaptive Methods","children":[{"content":"Adagrad","children":[],"payload":{"tag":"li","lines":"32,33"}},{"content":"RMSprop","children":[],"payload":{"tag":"li","lines":"33,34"}},{"content":"Adam","children":[{"content":"AdamW","children":[],"payload":{"tag":"li","lines":"35,36"}},{"content":"AMSGrad","children":[],"payload":{"tag":"li","lines":"36,37"}},{"content":"Nadam","children":[],"payload":{"tag":"li","lines":"37,38"}}],"payload":{"tag":"li","lines":"34,38"}}],"payload":{"tag":"li","lines":"31,38"}},{"content":"Second-Order Methods","children":[{"content":"Newton&#x2019;s Method","children":[],"payload":{"tag":"li","lines":"39,40"}},{"content":"L-BFGS","children":[],"payload":{"tag":"li","lines":"40,41"}},{"content":"Hessian-Free Optimization","children":[],"payload":{"tag":"li","lines":"41,43"}}],"payload":{"tag":"li","lines":"38,43"}}],"payload":{"tag":"h2","lines":"26,27"}},{"content":"3. Learning Rate Strategies","children":[{"content":"Constant LR","children":[],"payload":{"tag":"li","lines":"44,45"}},{"content":"Decay-Based","children":[{"content":"Step Decay","children":[],"payload":{"tag":"li","lines":"46,47"}},{"content":"Exponential Decay","children":[],"payload":{"tag":"li","lines":"47,48"}}],"payload":{"tag":"li","lines":"45,48"}},{"content":"Advanced Scheduling","children":[{"content":"Cosine Annealing","children":[],"payload":{"tag":"li","lines":"49,50"}},{"content":"Warm-up &amp; Restarts","children":[],"payload":{"tag":"li","lines":"50,51"}},{"content":"One-Cycle Policy","children":[],"payload":{"tag":"li","lines":"51,53"}}],"payload":{"tag":"li","lines":"48,53"}}],"payload":{"tag":"h2","lines":"43,44"}},{"content":"4. Regularization Techniques","children":[{"content":"L1 and L2 Regularization","children":[],"payload":{"tag":"li","lines":"54,55"}},{"content":"Dropout","children":[],"payload":{"tag":"li","lines":"55,56"}},{"content":"Batch Normalization","children":[],"payload":{"tag":"li","lines":"56,57"}},{"content":"Data Augmentation","children":[],"payload":{"tag":"li","lines":"57,58"}},{"content":"Early Stopping","children":[],"payload":{"tag":"li","lines":"58,60"}}],"payload":{"tag":"h2","lines":"53,54"}},{"content":"5. Initialization Methods","children":[{"content":"Xavier Initialization","children":[],"payload":{"tag":"li","lines":"61,62"}},{"content":"He Initialization","children":[],"payload":{"tag":"li","lines":"62,63"}},{"content":"Orthogonal Initialization","children":[],"payload":{"tag":"li","lines":"63,65"}}],"payload":{"tag":"h2","lines":"60,61"}},{"content":"6. Optimization Challenges","children":[{"content":"Vanishing Gradients","children":[],"payload":{"tag":"li","lines":"66,67"}},{"content":"Exploding Gradients","children":[],"payload":{"tag":"li","lines":"67,68"}},{"content":"Saddle Points","children":[],"payload":{"tag":"li","lines":"68,69"}},{"content":"Sharp vs. Flat Minima","children":[],"payload":{"tag":"li","lines":"69,70"}},{"content":"Local vs. Global Optima","children":[],"payload":{"tag":"li","lines":"70,72"}}],"payload":{"tag":"h2","lines":"65,66"}},{"content":"7. Hyperparameter Optimization","children":[{"content":"Grid Search","children":[],"payload":{"tag":"li","lines":"73,74"}},{"content":"Random Search","children":[],"payload":{"tag":"li","lines":"74,75"}},{"content":"Bayesian Optimization","children":[],"payload":{"tag":"li","lines":"75,76"}},{"content":"Hyperband","children":[],"payload":{"tag":"li","lines":"76,77"}},{"content":"Population-Based Training (PBT)","children":[],"payload":{"tag":"li","lines":"77,78"}},{"content":"Tools","children":[{"content":"Optuna","children":[],"payload":{"tag":"li","lines":"79,80"}},{"content":"Ray Tune","children":[],"payload":{"tag":"li","lines":"80,81"}},{"content":"Keras Tuner","children":[],"payload":{"tag":"li","lines":"81,83"}}],"payload":{"tag":"li","lines":"78,83"}}],"payload":{"tag":"h2","lines":"72,73"}},{"content":"8. Task-Specific Optimization","children":[{"content":"CNNs","children":[{"content":"Feature Optimization","children":[],"payload":{"tag":"li","lines":"85,86"}},{"content":"Filter Regularization","children":[],"payload":{"tag":"li","lines":"86,87"}}],"payload":{"tag":"li","lines":"84,87"}},{"content":"RNNs / LSTMs","children":[{"content":"Truncated BPTT","children":[],"payload":{"tag":"li","lines":"88,89"}},{"content":"Gradient Clipping","children":[],"payload":{"tag":"li","lines":"89,90"}}],"payload":{"tag":"li","lines":"87,90"}},{"content":"Transformers","children":[{"content":"LayerNorm Effect","children":[],"payload":{"tag":"li","lines":"91,92"}},{"content":"LR Warmup","children":[],"payload":{"tag":"li","lines":"92,94"}}],"payload":{"tag":"li","lines":"90,94"}}],"payload":{"tag":"h2","lines":"83,84"}},{"content":"9. Architecture-Level Optimization","children":[{"content":"Neural Architecture Search (NAS)","children":[{"content":"Reinforcement Learning-Based","children":[],"payload":{"tag":"li","lines":"96,97"}},{"content":"Gradient-Based NAS","children":[],"payload":{"tag":"li","lines":"97,98"}},{"content":"Evolutionary NAS","children":[],"payload":{"tag":"li","lines":"98,99"}}],"payload":{"tag":"li","lines":"95,99"}},{"content":"Pruning &amp; Quantization","children":[],"payload":{"tag":"li","lines":"99,100"}},{"content":"Quantization-Aware Optimization","children":[],"payload":{"tag":"li","lines":"100,102"}}],"payload":{"tag":"h2","lines":"94,95"}},{"content":"10. Advanced &amp; Emerging Topics","children":[{"content":"Sharpness-Aware Minimization (SAM)","children":[],"payload":{"tag":"li","lines":"103,104"}},{"content":"Curriculum Learning","children":[],"payload":{"tag":"li","lines":"104,105"}},{"content":"Meta-Learning Optimization","children":[],"payload":{"tag":"li","lines":"105,106"}},{"content":"Test-Time Training","children":[],"payload":{"tag":"li","lines":"106,107"}},{"content":"Loss Landscape Analysis","children":[],"payload":{"tag":"li","lines":"107,108"}},{"content":"Fairness-Aware Optimization","children":[],"payload":{"tag":"li","lines":"108,109"}},{"content":"Continual / Multi-task Optimization","children":[{"content":"Elastic Weight Consolidation (EWC)","children":[],"payload":{"tag":"li","lines":"110,111"}},{"content":"Gradient Balancing (GradNorm)","children":[],"payload":{"tag":"li","lines":"111,112"}}],"payload":{"tag":"li","lines":"109,112"}},{"content":"Prompt-Tuning &amp; LoRA","children":[],"payload":{"tag":"li","lines":"112,113"}},{"content":"Zero-shot / Few-shot Optimization","children":[],"payload":{"tag":"li","lines":"113,115"}}],"payload":{"tag":"h2","lines":"102,103"}},{"content":"11. Scalable &amp; Distributed Optimization","children":[{"content":"Federated Optimization","children":[{"content":"FedAvg","children":[],"payload":{"tag":"li","lines":"117,118"}},{"content":"FedProx","children":[],"payload":{"tag":"li","lines":"118,119"}}],"payload":{"tag":"li","lines":"116,119"}},{"content":"Distributed Optimizers","children":[{"content":"ZeRO (Zero Redundancy Optimizer)","children":[],"payload":{"tag":"li","lines":"120,121"}},{"content":"Megatron-LM Optimizer Partitioning","children":[],"payload":{"tag":"li","lines":"121,123"}}],"payload":{"tag":"li","lines":"119,123"}}],"payload":{"tag":"h2","lines":"115,116"}},{"content":"12. Differentiable Programming &amp; AutoML","children":[{"content":"Differentiable Programming in PyTorch / JAX","children":[],"payload":{"tag":"li","lines":"124,125"}},{"content":"Differentiable NAS (e.g., DARTS)","children":[],"payload":{"tag":"li","lines":"125,126"}},{"content":"AutoML Optimization Loops","children":[],"payload":{"tag":"li","lines":"126,127"}}],"payload":{"tag":"h2","lines":"123,124"}}],"payload":{"tag":"h1","lines":"9,10"}},{"colorFreezeLevel":0,"initialExpandLevel":1,"activeNode":{"placement":"center"}})</script>
</body>
</html>
